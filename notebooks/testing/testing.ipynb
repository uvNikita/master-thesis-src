{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import caffe\n",
    "import itertools\n",
    "import pandas\n",
    "import cv2\n",
    "import scipy\n",
    "\n",
    "from os.path import join as pjoin\n",
    "from math import ceil\n",
    "from copy import copy\n",
    "from collections import OrderedDict\n",
    "from operator import itemgetter, attrgetter\n",
    "from matplotlib import pyplot\n",
    "from scipy import ndimage as ndimage\n",
    "\n",
    "from caffe import layers as L\n",
    "from caffe import params as P\n",
    "import tools\n",
    "\n",
    "from ntb.layer.data import Transformer\n",
    "from ntb.db import *\n",
    "\n",
    "%matplotlib inline\n",
    "pyplot.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# set seed to make randomization reproducible\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db = NTBDB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(self, definition, weights, labels, transformer):\n",
    "        self.net = caffe.Net(definition, weights, caffe.TEST)\n",
    "        self.transformer = transformer\n",
    "        self.labels = labels\n",
    "\n",
    "    def get_scores(self, indexes, batch_size=256):\n",
    "        data_layer = self.net.blobs['data']\n",
    "        shape = self.net.blobs['data'].shape\n",
    "        shape[0] = batch_size\n",
    "        data_layer.reshape(*shape)\n",
    "\n",
    "        def predict_batch(batch):\n",
    "            for i, index in enumerate(batch):\n",
    "                image = caffe.io.load_image(image_path(db.metadata[index]))\n",
    "                self.net.blobs['data'].data[...][i] = self.transformer.preprocess(image)\n",
    "            self.net.forward()\n",
    "\n",
    "        def get_batch_scores(batch):\n",
    "            res = []\n",
    "            for i, index in enumerate(batch):\n",
    "                row = list()\n",
    "                row.insert(0, index)\n",
    "                yield (index,) + tuple(self.net.blobs['score'].data[i])\n",
    "\n",
    "        batch_num = int(ceil(indexes.size/float(batch_size)))\n",
    "        scores = []\n",
    "        print \"Going to process\", batch_num, \"batches\"\n",
    "        for i, batch in enumerate(np.array_split(indexes, batch_num)):\n",
    "            predict_batch(batch)\n",
    "            scores.extend(get_batch_scores(batch))\n",
    "            if i % 50 == 0:\n",
    "                print \"Batch\", i, \"finised\"\n",
    "\n",
    "        dtype = [(label, 'f4') for label in labels]\n",
    "        dtype.insert(0, ('idx', 'S27'))\n",
    "        return np.array(scores, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_curves(labels, scores, ground_truth):\n",
    "    def get_label_curve(label):\n",
    "        sorted_images = np.sort(scores, order=[label], axis=0)[['idx']][::-1].copy().view('S27')\n",
    "\n",
    "        res = []\n",
    "        predicted_true = 0\n",
    "        true_positive = 0\n",
    "        for index in sorted_images:\n",
    "            predicted_true += 1\n",
    "            if index in ground_truth[label]:\n",
    "                true_positive += 1\n",
    "            p = float(true_positive) / predicted_true\n",
    "            r = float(true_positive) / len(ground_truth[label])\n",
    "            res.append((p, r))\n",
    "        return np.array(res, dtype=[('precision', 'f4'), ('recall', 'f4')])\n",
    "\n",
    "    return {label: get_label_curve(label) for label in labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_average_precision(curves):\n",
    "    recall_range = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "    res = {}\n",
    "    for label, label_curve in curves.iteritems():\n",
    "        s = 0\n",
    "        for recall in recall_range:\n",
    "            print label, recall\n",
    "            s += label_curve[label_curve['recall'] >= recall]['precision'].max()\n",
    "        res[label] = s / len(recall_range)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NETS_DIR = os.path.join(\"/storage/ntb/nets\")\n",
    "def get_scores_for_net(net_name, transformer, snapshot_num, batch_size=256):\n",
    "    scores_file_path = pjoin(data_dir, 'scores_{}.np'.format(snapshot_num))\n",
    "    try:\n",
    "        with open(scores_file_path) as scores_file:\n",
    "            return np.load(scores_file)\n",
    "    except IOError:\n",
    "        pass\n",
    "\n",
    "    net_dir = pjoin(NETS_DIR, net_name)\n",
    "    data_dir = pjoin(net_dir, 'data')\n",
    "    labels = np.load(pjoin(data_dir, 'labels.np'))\n",
    "    net = Network(\n",
    "        pjoin(net_dir, 'testnet.prototxt'),\n",
    "        pjoin(net_dir, 'snapshots', 'snapshot_iter_{}.caffemodel'.format(snapshot_num)),\n",
    "        labels,\n",
    "        transformer,\n",
    "    )\n",
    "    with open(pjoin(data_dir, 'test.pickle')) as test_file:\n",
    "        test_data = pickle.load(test_file)\n",
    "    assert set(test_data.keys()) == set(labels)\n",
    "    indexes = np.unique(np.concatenate(test_data.values()))\n",
    "    scores = net.get_scores(indexes, batch_size=batch_size)\n",
    "    \n",
    "    with open(scores_file_path, mode='w') as scores_file:\n",
    "        scores.dump(scores_file)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid loss function, no augmentation, raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = get_scores_for_net(\"ft_sigmoid_noaug\", Transformer(shape=[227,227]), 853)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net_name = \"ft_sigmoid_noaug\"\n",
    "net_dir = pjoin(NETS_DIR, net_name)\n",
    "data_dir = pjoin(net_dir, 'data')\n",
    "labels = np.load(pjoin(data_dir, 'labels.np'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = Network(\n",
    "    pjoin(net_dir, 'testnet.prototxt'),\n",
    "    pjoin(net_dir, 'snapshots', 'snapshot_iter_853.caffemodel'),\n",
    "    labels,\n",
    "    Transformer(shape=[227,227]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(pjoin(data_dir, 'test.pickle')) as test_file:\n",
    "    test_data = pickle.load(test_file)\n",
    "assert set(test_data.keys()) == set(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indexes = np.unique(np.concatenate(test_data.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to process 134 batches\n",
      "Batch 0 finised\n",
      "Batch 50 finised\n",
      "Batch 100 finised\n",
      "CPU times: user 1h 6min 49s, sys: 4min 19s, total: 1h 11min 8s\n",
      "Wall time: 54min 35s\n"
     ]
    }
   ],
   "source": [
    "%time scores = net.get_scores(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores_file_path = pjoin(data_dir, 'scores_{}.np'.format(853))\n",
    "with open(scores_file_path, mode='w') as scores_file:\n",
    "    scores.dump(scores_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "curves = get_curves(labels, scores, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34245"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
